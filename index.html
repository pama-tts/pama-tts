
<html>	
  <head>	
    <meta charset="UTF-8">	
    <title>Audio samples from "PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS With Accurate Phoneme Duration Control"</title>	
    <link rel="stylesheet" type="text/css" href="stylesheet.css"/>	
    <link rel="shortcut icon" href="mi.png">	
  </head>	
  <body>	
    <div>	
      <article>	
        <header>	
          <h1>Audio samples from "PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS With Accurate Phoneme Duration Control"</h1>	
        </header>	
      </article>	

      <p><b>Paper: </b><a href="https://">arXiv</a></p>	
      <p><b>Authors: </b>Yunchao He, Jian Luan, Yujun Wang</p>	

      <p><b>Abstract:</b>	
       Sequence expansion between encoder and decoder is a critical challenge in sequence-to-sequence TTS. Attention-based methods achieve great naturalness but suffer from the unstable issues like missing and repeating phonemes, not to mention accurate local duration control. Duration-informed methods, on the contrary, seem easy to adjust phoneme duration but show obvious degradation in speech naturalness. This paper proposes PAMA-TTS to address the problem. It takes the advantage of both flexible attention and explicit duration model. Based on a monotonic attention mechanism, PAMA-TTS also leverages the token duration and countdown information, i.e. in how many frames the present phoneme will end. They help the attention to move forward along token sequence in a soft but reliable control. 
 </p>	

      <h2>Female Speaker</h2>	
      <p><i>Random sample from test set.</i></p>	

       

    </div>	
  </body>	
</html>	
